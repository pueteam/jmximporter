# Specify server locations in a SOLR_LOCATOR variable;
# used later in variable substitutions
# Change the zkHost to point to your own Zookeeper quorum
SOLR_LOCATOR : {
    # Name of solr collection
    collection : kafka-metrics
    # ZooKeeper ensemble
    zkHost : "puemaster1.pue.es:2181,pueworker1.pue.es:2181,pueworker2.pue.es:2181/solr"
}

# Specify an array of one or more morphlines, each of which defines an ETL
# transformation chain. A morphline consists of one or more (potentially
# nested) commands. A morphline is a way to consume records (e.g. Flume events,
# HDFS files or blocks), turn them into a stream of records, and pipe the stream
# of records through a set of easily configurable transformations on it's way to
# Solr (or a MapReduceIndexerTool RecordWriter that feeds via a Reducer into Solr).
morphlines : [
{
    # Name used to identify a morphline. E.g. used if there are multiple morphlines in a
    # morphline config file
    id : morphline1
    # Import all morphline commands in these java packages and their subpackages.
    # Other commands that may be present on the classpath are not visible to this morphline.
    importCommands : ["org.kitesdk.**", "org.apache.solr.**"]
    commands : [
    # { logInfo { format : "BODY : {}", args : ["@{}"] } }
    { readJson { outputClass : com.fasterxml.jackson.databind.JsonNode } }
    {
        extractJsonPaths {
          flatten : true
          paths : {
            name: "/name"
	    timestamp: "/timestamp"
            mbean: "/mbean"
            attributes: "/attributes"
            type: "/type"
            rule: "/rule"
            values: "/values"
            labels_topic: "/labels/topic"
            labels_partition: "/labels/partition"
            labels_request: "/labels/request"
            labels_clientId: "/labels/clientId"
            labels_broker: "/labels/broker"
          }
        }
    }
    #{ logInfo { format : "BODY : {}", args : ["@{}"] } }
    {
	    setValues {
               _attachment_body : []
               _attachment_mimetype : []
        }
    }

    {
        tryRules {
            throwExceptionIfAllRulesFailed : true
            rules : [
                {
                    commands: [
                        #{ logInfo { format : "hello geoIP" } }
                        { generateUUID { field:id } }
                    ]
                }
            ]
        }
    }

    # convert the timestamp field to "yyyy-MM-dd'T'HH:mm:ss.SSSZ" format
    {
       ##  21/Nov/2014:22:08:27
        convertTimestamp {
            field : timestamp
            inputFormats : ["unixTimeInMillis", "yyyy-MM-dd'T'HH:mm:ss.SSSSSSX"]
            ##inputTimezone : Europe/Madrid
           outputFormat : "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
           outputTimezone : UTC
        }
    }

    # Consume the output record of the previous command and pipe another
    # record downstream.
    #
    # This command sanitizes record fields that are unknown to Solr schema.xml
    # by deleting them. Recall that Solr throws an exception on any attempt to
    # load a document that contains a field that isn't specified in schema.xml
    {
        sanitizeUnknownSolrFields {
            # Location from which to fetch Solr schema
            solrLocator : ${SOLR_LOCATOR}
        }
    }

    # load the record into a SolrServer or MapReduce SolrOutputFormat.
    {
        loadSolr {
            solrLocator : ${SOLR_LOCATOR}
        }
    }
    ]
}
]